- There are two types of **application communication**: 
	- **Synchronous communication**: This may be a problem if there are sudden spikes in traffic, in which case you use...
	- **Asynchronous/event-based communication**: Decoupled application that communicate through a **queue (SQS)**, **pub/sub (SNS)**, **real-time messaging (Kinesis)** -- these services can scale independently

## Queue (SQS)

![[Pasted image 20241016145604.png]]
- **Message**: Element of a queue
	- Produced using **SendMessage API**
	- Example: an order to be processed can contain order id, customer id, etc. in a message
	- Messages are **consumed** by **consumers (EC2 instances, servers, etc.)** when they **poll** SQS for messages (<= 10 at a time), **process** them, and **delete** them using **DeleteMessageAPI**
	- Consumers can process messages in **parallel** and increase throughput with [[5. High Availability and Scalability#Horizontal Scalability|horizontal scaling]]
		- Use an [[5. High Availability and Scalability#Auto Scaling Group|ASG]] 

![[Pasted image 20241016150549.png]]


- **Standard Queue**
	- Unlimited throughput/messages in queue
	- Retains messages for **default 4 days**, **max 14 days**
	- <10ms latency for publish/receive
	- **<= 256KB** per message
		- Use the **SQS Extended Client** (Java Library) to send bigger messages
			- Uses S3 bucket to store messages
### Security

- **In-Flight Encryption** is provided by HTTPS
- **At-rest Encryption** is provided by KMS keys
- There is an option for **client-side encryption**
- **IAM policies** can regulate access to the SQS API 
- **SQS Access Policies (like [[9. Amazon S3 Introduction#Bucket Policies|bucket policies]])** allow cross-account access to SQS queues
	- Good for allowing other services to write to an SQS queue 
### Message Visibility Timeout

- **Timeout** is how long the message becomes invisible after a consumer polls it 
	- Default is **30s**, after that the message is **visible in queue** again
	- If message is not processed in visibility timeout, it'll be **processed twice** -- you can fix this by calling the **ChangeMessageVisibility API** to get more time 
		- Too high = long processing time
		- Too low = duplicates
- Can also **delay** a message **<= 15 min**; use the **DelaySeconds** parameter
- **Long polling** allows a client to wait for messages if there aren't any in the queue
	- Decreases API calls (increases efficiency/decreases latency)
	- 1-20s
	- Use the **ReceiveMessageWaitTimeSeconds API** for this

### Dead Letter Queue 

- After a message goes in the queue a **certain amount of times (editable)**, the message goes in a **Dead Letter Queue (DLQ)**
	- Good for debugging
	- DLQ of a queue must be **same type of queue**
	- After the message expires in DLQ, it is **permanently deleted**
- **Redrive to Source**: Good for putting messages from DLQ back in queue after debugging 

### APIs to Know

- **CreateQueue, DeleteQueue**: self-explanatory
- **PurgeQueue**: Delete all messages in queue 
- **SendMessage, ReceiveMessage, DeleteMessage**
- **MaxNumberOfMessages**: amount of messages per poll
- **ReceiveMessageWaitTimeSeconds**: Long <--> Short Polling
- **ChangeMessageVisibility**: Change message timeout

### FIFO Queue 

- Uses **FIFO** ordering instead of random
	- **Not that good throughput**
- Guarantees no duplicate processing with **exactly-once send capability**
	- Uses **deduplication** by assigning a MessageDeduplicationId to a message and not delivering any other message with the same ID in the next **5 minutes**
- If you want organize message order by **subset**, you can use **message grouping** and assign an ID to a group of messages -> assign to a consumer
	- **Ordering across groups is not guaranteed**

## Pub/Sub (SNS)

- **SNS** is good for sending **one message -> many receivers**
	- **Topic** is core element of SNS; a **publisher** sends a message to a **topic** and the topic sends the message to every **subscriber** 
		- <= 12.5m subs/topic
		- <= 100k topics
	- A lot of AWS services **integrate** with SNS for notifications
- There are two ways to **publish**: 
	- **Topic Publish**: Create a topic, create subscription(s), publish to topic
	- **Direct Publish**: Create an app, create an endpoint, publish to endpoint

### Security

- Same as [[#Security|SQS Security]] 

### Fan Out

- Can combine **SQS w/ SNS** where there are queues subscribed to a topic
	- Fully decoupled
	- Allows for data persistence, delayed processing, retries
	- Can **add SQS subscribers** over time
	- Ensure SQS queue policy **allows SNS**
	- Works **cross-region**
- Example: S3 only allows sending to one service (one S3 Event Rule)
	- S3 -> SNS -> SQS Queues

### FIFO Queue

- Same thing as [[#FIFO Queue|SQS FIFO Queue]]
- Can have both SQS Standard + FIFO Queues as subscribers

### Message Filtering

- You can use a **JSON Policy** to filter message sent to subscription
	- Subscription receives **all messages by default**

## Kinesis 

- Good for processing **streaming data in real-time**
	- Application logs, metrics, telemetry data
- **Kinesis Data Streams**: Capture/process/store data streams
- **Kinesis Data Firehose**: Data Streams -> Data stores
- **Kinesis Data Analytics**: Analyze data streams w/ SQL 
- **Kinesis Video Streams**: Capture/process/store video streams

### Kinesis Data Streams

![[Pasted image 20241016163126.png]]

- **Shard**: Basic unit of Kinesis; partition of data that defines capacity/data ordering of stream ^befd5b
- **Retention Period**: 1d-1y
- Can **reprocess** data
- Data inserted in kinesis **cannot be deleted**
- Data in the **same partition -> same shard**
- **Producers** of data streams can be AWS SDK, Kinesis Producer Library (KPL), Kinesis Agent
	- Puts a **data record** (sequence number, partition key, data blob <= 1MB)
	- Throughput of 1MB/s
	- Uses the **PutRecord API**
	- Able to reduce costs with **batching**
- **Consumers** can be Kinesis Client Library (KCL), AWS SDK
	- Managed services are AWS Lambda, Kinesis Data Firehose, Kinesis Data Analytics
	- **Shared Fan-Out Consumer (Pull)**: Low number of consuming apps, 2MB/s per shard, <= 5 GetRecords calls/s
		- Higher latency
		- Cheaper
		- Returns <= 10MB, throttled for 5s
	- **Enhanced Fan-Out Consumer (Push)**: Multiple consuming apps for same stream, 2MB/s per customer per shard
		- Lower latency
		- More expensive 
		- Uses SubscribeToShard API 
		- 5 consumer applications per data stream (10 MB/s per stream)
	- **AWS Lambda** supports both these types; reads records in **batches** (configurable)
		- If error happens, Lambda retries until it succeeds OR data expires
		- <= 10 batches/shard at the same time
	- **KCL** is a Java library that helps read records from KDS with load balancing
		- 1 shard can **only be read by 1 KCL**
		- Progress is tracked in DynamoDB
- **Capacity Modes** include
	- **Provisioned mode**: Choose # of shards provisioned (scale manually/automatically using API)
		- Each shard gets 1MB/s in, 2MB/s out
		- Pay shards/hr
	- **On-Demand Mode**: Has a default amount of shards provisioned (4MB/s) and scales based on throughput
		- Pay stream/hr (data I/O per GB)
- **Security**: Similar to SQS and SNS security
	- Kinesis is able to access [[8. Amazon VPC|VPC Endpoints]] 
	- Monitor API calls with CloudTrail
- **Shard Splitting**: Increase the stream capacity (1MB/s per shard)
	- Good for splitting a "hot shard"; old shard will be deleted after data expires
	- **Not automatic**
	- Can only **split into two shards** in a single operation
- **Shard Merging**: Opposite of splitting; good for grouping two shards with low traffic
### Kinesis Firehose

- **Firehose** is a **fully managed service** that can send data streams to data stores
	- AWS: Redshift/S3/OpenSearch
	- 3rd Party: Splunk/MongoDB/DataDog/NewRelic
- Near **real time**
	- 0-900s
- Supports plenty of stuff
- Can send failed data -> bucket
### Kinesis Data Analytics

- Real-time analytics on Data Streams + Firehose
- Add reference data from S3 to streaming data
- Outputs **Kinesis Data Streams** or **Firehose**
- Good for time-series analytics, real-time dashboards/metrics
- You can use **Apache Flink** to analyze streaming data 
	- Provision resources, parallel computation
	- Backup apps
	- Remember that Flink **does not read from Firehose**; you should use Kinesis Analytics instead


